{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "import h5py\n",
    "import re\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import scipy.io\n",
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Class for storing and working with the Brain Tumor\n",
    "# dataset. Uses h5py to work with the .mat files\n",
    "# 3064 samples in the dataset\n",
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        \n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i in range(1,3065):\n",
    "            data = self.load_mat(data_path + str(i) + \".mat\")\n",
    "            \n",
    "            try:\n",
    "                label = data[1]\n",
    "                mask = data[2]\n",
    "                \n",
    "                image = data[0]\n",
    "                \n",
    "                # applying tumor mask\n",
    "#                 image = image * mask\n",
    "                \n",
    "                self.images.append(image)\n",
    "                self.labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during processing file {i}: {e}\")\n",
    "    \n",
    "    \n",
    "    def load_mat(self, file_path):\n",
    "        data=h5py.File(file_path, 'r')\n",
    "        image = data.get('cjdata/image/')\n",
    "        label = data.get('cjdata/label/')\n",
    "        mask = data.get('cjdata/tumorMask/')\n",
    "        image = np.array(image, dtype=\"float32\")\n",
    "        label = np.array(label, dtype=\"int\")\n",
    "        mask = np.array(mask, dtype=\"int\")\n",
    "        return image, label, mask\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = torch.tensor(self.images[idx], dtype=torch.float32)\n",
    "        image = image.expand(3, -1, -1)\n",
    "        image = F.interpolate(image.unsqueeze(0), size=(512, 512), mode='bilinear', align_corners=False).squeeze(0)\n",
    "        \n",
    "        label = torch.tensor(self.labels[idx][0], dtype=torch.long)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Function for extracting features using a model.\n",
    "# Flattens the features and then stacks them before\n",
    "# returning.\n",
    "def extract_features(model, dataloader):\n",
    "    \n",
    "    features = []\n",
    "    return_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc='Extracting Features'):\n",
    "            outputs = model(inputs)\n",
    "            flattened_features = outputs.view(outputs.size(0), -1)\n",
    "            features.append(flattened_features.numpy())\n",
    "            return_labels.append(labels.numpy())\n",
    "\n",
    "    return_labels = np.concatenate(return_labels)\n",
    "    features = np.vstack(features)\n",
    "    return features, return_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Function for evaluating classifiers and printing\n",
    "# the required metrics\n",
    "def evaluate_classifier(y_true, y_pred, model_name):\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Precision\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Recall\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    y_true_one_hot_encoding = label_binarize(y_true, classes=np.unique(y_true))\n",
    "\n",
    "    if len(y_pred.shape) == 1:\n",
    "        y_pred_one_hot_encoding = label_binarize(y_pred, classes=np.unique(y_true))\n",
    "    else:\n",
    "        y_pred_one_hot_encoding = y_pred\n",
    "\n",
    "    # AUC-ROC\n",
    "    roc_auc = roc_auc_score(y_true_one_hot_encoding, y_pred_one_hot_encoding, average='weighted')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"------------------------------------------------------\")\n",
    "    print(f\"Evaluation results for {model_name}:\\n\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "    print(f\"------------------------------------------------------\")\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset size:  2451\n",
      "Validation Dataset size:  306\n",
      "Test Dataset size:  307\n"
     ]
    }
   ],
   "source": [
    "# Load the data and split into train/validation/test sets\n",
    "\n",
    "dataset = BrainTumorDataset('/home/rit/temp/dataset/')\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "print(\"Train Dataset size: \", train_size)\n",
    "print(\"Validation Dataset size: \", val_size)\n",
    "print(\"Test Dataset size: \", test_size)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Creating dataloaders\n",
    "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Extraction using ResNet-18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rit/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rit/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Extracting Features: 100%|██████████| 77/77 [00:49<00:00,  1.55it/s]\n",
      "Extracting Features: 100%|██████████| 10/10 [00:06<00:00,  1.61it/s]\n",
      "Extracting Features: 100%|██████████| 10/10 [00:06<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use a pre-trained ResNet18 for feature extraction\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "resnet18 = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Extacting the features\n",
    "train_features, train_labels = extract_features(resnet18, trainloader)\n",
    "test_features, test_labels = extract_features(resnet18, testloader)\n",
    "val_features, val_labels = extract_features(resnet18, valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Bayesian Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Evaluation results for Bayesian:\n",
      "\n",
      "Accuracy: 0.8137\n",
      "Precision: 0.8239\n",
      "Recall: 0.8137\n",
      "F1 Score: 0.8160\n",
      "ROC AUC Score: 0.8620\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 55   3   9]\n",
      " [ 22 115   8]\n",
      " [  4  11  79]]\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rit/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Declaring the Bayesian Classifier\n",
    "bayesian_classifier = GaussianNB()\n",
    "\n",
    "bayesian_classifier.fit(train_features, train_labels)\n",
    "\n",
    "val_preds_bayesian = bayesian_classifier.predict(val_features)\n",
    "\n",
    "evaluate_classifier(val_labels, val_preds_bayesian, 'Bayesian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Evaluation results for Decision Tree:\n",
      "\n",
      "Accuracy: 0.7320\n",
      "Precision: 0.7329\n",
      "Recall: 0.7320\n",
      "F1 Score: 0.7324\n",
      "ROC AUC Score: 0.7932\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 37  19  11]\n",
      " [ 19 115  11]\n",
      " [ 12  10  72]]\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Declaring the Decision Tree Classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "decision_tree_classifier.fit(train_features, train_labels)\n",
    "\n",
    "val_preds_tree = decision_tree_classifier.predict(val_features)\n",
    "\n",
    "evaluate_classifier(val_labels, val_preds_tree, 'Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SVM (Support Vector Machines) Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rit/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Evaluation results for Support Vector Machine (SVM):\n",
      "\n",
      "Accuracy: 0.9314\n",
      "Precision: 0.9350\n",
      "Recall: 0.9314\n",
      "F1 Score: 0.9324\n",
      "ROC AUC Score: 0.9532\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 59   1   7]\n",
      " [ 10 135   0]\n",
      " [  3   0  91]]\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Declaring the SVM Classifier\n",
    "svm_classifier = SVC(probability=True)\n",
    "\n",
    "svm_classifier.fit(train_features, train_labels)\n",
    "\n",
    "val_preds_svm = svm_classifier.predict(val_features)\n",
    "\n",
    "evaluate_classifier(val_labels, val_preds_svm, 'Support Vector Machine (SVM)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
